# An example of ClusterConfig object using an existing VPC:
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-eks
  region: us-west-2

vpc:
  id: "vpc-06e3f18ed91510898"  # (optional, must match VPC ID used for each subnet below)
  cidr: "172.16.0.0/16"       # (optional, must match CIDR used by the given VPC)
  subnets:
    # must provide 'private' and/or 'public' subnets by availibility zone as shown
    public:
      us-west-2a:
        id: "subnet-0572e450eab1410f1"
        cidr: "172.16.192.0/20" # (optional, must match CIDR used by the given subnet)

      us-west-2b:
        id: "subnet-012b42fedb0c077f2"
        cidr: "172.16.208.0/20"  # (optional, must match CIDR used by the given subnet)

      us-west-2c:
        id: "subnet-0a737fd8098403f7f"
        cidr: "172.16.224.0/20"   # (optional, must match CIDR used by the given subnet)

managedNodeGroups:
  - name: managed-ng-statefull
    instanceType: t2.medium
    minSize: 2
    desiredCapacity: 3
    maxSize: 4
    availabilityZones: ["us-west-2a", "us-west-2b", "us-west-2c"]
    volumeSize: 10
    ssh:
      allow: true
      publicKeyPath: ~/.ssh/id_rsa.pub
      # new feature for restricting SSH access to certain AWS security group IDs
      sourceSecurityGroupIds: ["sg-09b39ebf731186768"]
    labels: {role: worker}
    tags:
      nodegroup-role: worker
    iam:
      withAddonPolicies:
        externalDNS: true
        certManager: true

  - name: managed-ng-stateless
    instanceType: t2.medium
    # launch nodegroup in private subnets
    # privateNetworking: true

  

